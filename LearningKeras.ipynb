{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"LearningKeras.ipynb","provenance":[],"authorship_tag":"ABX9TyN8yWgVdn+ngmhQEvH3pz0P"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"IrtPyUryxvJV","executionInfo":{"status":"ok","timestamp":1636832038534,"user_tz":300,"elapsed":175,"user":{"displayName":"sanyam sood","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giwr9-AjWHYVbeGZ5reOOO2gvUC9sL5Byn8qrEdpg=s64","userId":"00322300153673038775"}}},"source":["#Making a sequential Model\n","from tensorflow.keras.models import Sequential\n","model = Sequential()\n"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"TrSWGyPYyvE8","executionInfo":{"status":"ok","timestamp":1636832151700,"user_tz":300,"elapsed":164,"user":{"displayName":"sanyam sood","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giwr9-AjWHYVbeGZ5reOOO2gvUC9sL5Byn8qrEdpg=s64","userId":"00322300153673038775"}}},"source":["#Stacking layers is as easy as .add():\n","from tensorflow.keras.layers import Dense\n","\n","model.add(Dense(units=64, activation='relu'))\n","model.add(Dense(units=10, activation='softmax'))\n"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"kVaWI09-zL3-","executionInfo":{"status":"ok","timestamp":1636832229681,"user_tz":300,"elapsed":517,"user":{"displayName":"sanyam sood","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giwr9-AjWHYVbeGZ5reOOO2gvUC9sL5Byn8qrEdpg=s64","userId":"00322300153673038775"}}},"source":["#Once the model looks good, configure its learning process with .compile()\n","model.compile(loss='categorical_crossentropy',\n","              optimizer='sgd',\n","              metrics=['accuracy'])\n"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"h64hBV7i1Fyr","executionInfo":{"status":"ok","timestamp":1636832684790,"user_tz":300,"elapsed":168,"user":{"displayName":"sanyam sood","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giwr9-AjWHYVbeGZ5reOOO2gvUC9sL5Byn8qrEdpg=s64","userId":"00322300153673038775"}}},"source":["#the ultimate control being the easy extensibility of the source code via subclassing\n","model.compile(loss=keras.losses.categorical_crossentropy,\n","              optimizer=keras.optimizers.SGD(learning_rate=0.01, momentum=0.9, nesterov=True))"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"n4XCBDTG1TuJ"},"source":["#You can now iterate on your training data in batches:\n","# x_train and y_train are Numpy arrays\n","model.fit(x_train, y_train, epochs=5, batch_size=32)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v80MJE3E1ZMh"},"source":["#Evaluate your test loss and metrics in one line:\n","loss_and_metrics = model.evaluate(x_test, y_test, batch_size=128)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C08AHEGV1fVU"},"source":["#Or generate predictions on new data:\n","classes = model.predict(x_test, batch_size=128)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W5_3-f3u1mdd"},"source":["#What you just saw is the most elementary way to use Keras.\n","\n","#However, Keras is also a highly-flexible framework suitable\n","#to iterate on state-of-the-art research ideas. \n","#Keras follows the principle of progressive disclosure of complexity: \n","#it makes it easy to get started, yet it makes it possible \n","#to handle arbitrarily advanced use cases, \n","#only requiring incremental learning at each step.\n","\n","#(In much the same way that you were able to train &\n"," #evaluate a simple neural network above in a few lines,\n","  #you can use Keras to quickly develop new training procedures\n","  # or exotic model architectures. \n","  # Here's a low-level training loop example, \n","  # combining Keras functionality with the TensorFlow GradientTape:)\n","  \n","  import tensorflow as tf\n","\n","# Prepare an optimizer.\n","optimizer = tf.keras.optimizers.Adam()\n","# Prepare a loss function.\n","loss_fn = tf.keras.losses.kl_divergence\n","\n","# Iterate over the batches of a dataset.\n","for inputs, targets in dataset:\n","    # Open a GradientTape.\n","    with tf.GradientTape() as tape:\n","        # Forward pass.\n","        predictions = model(inputs)\n","        # Compute the loss value for this batch.\n","        loss_value = loss_fn(targets, predictions)\n","\n","    # Get gradients of loss wrt the weights.\n","    gradients = tape.gradient(loss_value, model.trainable_weights)\n","    # Update the weights of the model.\n","    optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n"],"execution_count":null,"outputs":[]}]}