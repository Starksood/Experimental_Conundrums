{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CH5 Natural Language Processing.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPCq/YdpwpHkLdQS21AYnOq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Starksood/Experimental_Conundrums/blob/main/CH5_Natural_Language_Processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "iSe2a6YAe55a"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using out-of-vocabulary tokens\n",
        "One tool to use to handle these situations is an out-of-vocabulary (OOV) token."
      ],
      "metadata": {
        "id": "tXS88AVyj0XX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\n",
        "             'Today is a sunny day',\n",
        "             'Today is a rainy day',\n",
        "             'Will it rainy tommorow?'\n",
        "]\n",
        "\n",
        "tokenizer = Tokenizer(num_words = 100, oov_token=\"<OOV\")\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "\n",
        "print(word_index)\n",
        "print(sequences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQSL191ZgoHS",
        "outputId": "ed015fd1-a992-4a5c-c017-6e3a6fe7f164"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'<OOV': 1, 'today': 2, 'is': 3, 'a': 4, 'day': 5, 'rainy': 6, 'sunny': 7, 'will': 8, 'it': 9, 'tommorow': 10}\n",
            "[[2, 3, 4, 7, 5], [2, 3, 4, 6, 5], [8, 9, 6, 10]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "If you want to make these the same length, you can use the pad_sequences API. First,\n",
        "you’ll need to import it:\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "Using the API is very straightforward. To convert your (unpadded) sequences into a\n",
        "padded set, you simply call pad_sequences like this:\n",
        "padded = pad_sequences(sequences)\n",
        "print(padded)\n",
        "You’ll get a nicely formatted set of sequences."
      ],
      "metadata": {
        "id": "9zI9JVkXj4kJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if you want your sequences to be padded\n",
        "with zeros at the end, you can use:\n",
        "padded = pad_sequences(sequences, padding='post')"
      ],
      "metadata": {
        "id": "VBFSOY4ykEEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "use the maxlen parameter, specifying the desired maximum length, when calling pad_sequences, like\n",
        "this:\n",
        "padded = pad_sequences(sequences, padding='post', maxlen=6)\n",
        "You can\n",
        "override the default behavior with the truncating parameter, as follows:\n",
        "padded = pad_sequences(sequences, padding='post', maxlen=6, truncating='post')"
      ],
      "metadata": {
        "id": "dBzLlyl1kbgg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# if your sentences contain HTML tags such as <br>, they’ll be removed by this code:\n",
        "from bs4 import BeautifulSoup\n",
        "soup = BeautifulSoup(sentence)\n",
        "sentence = soup.get_text()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "id": "nWlJP5EHkzYr",
        "outputId": "54e61033-64ed-4517-b1c3-4418a5c4473a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-7e2ce8fdc4b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# if your sentences contain HTML tags such as <br>, they’ll be removed by this code:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbs4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'sentence' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess\n",
        "your sentences, removing instances of stopwords. Here’s an abbreviated example:\n",
        "stopwords = [\"a\", \"about\", \"above\", ... \"yours\", \"yourself\", \"yourselves\"]"
      ],
      "metadata": {
        "id": "ckF6G2HOleL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = sentence.split()\n",
        "filtered_sentence = \"\"\n",
        "for word in words:\n",
        "if word not in stopwords:\n",
        "filtered_sentence = filtered_sentence + word + \" \"\n",
        "sentences.append(filtered_sentence)"
      ],
      "metadata": {
        "id": "LjtNxeAElszo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "It also comes with a constant, string.punctuation, that contains a\n",
        "list of common punctuation marks, so to remove them from a word you can do the\n",
        "following:\n",
        "import string\n",
        "table = str.maketrans('', '', string.punctuation)\n",
        "words = sentence.split()\n",
        "filtered_sentence = \"\"\n",
        "for word in words:\n",
        "word = word.translate(table)\n",
        "if word not in stopwords:\n",
        "filtered_sentence = filtered_sentence + word + \" \"\n",
        "sentences.append(filtered_sentence)"
      ],
      "metadata": {
        "id": "R-4b1Ccol3C5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Note that by\n",
        "wrapping the tfds.load call in tfds.as_numpy you ensure that the data will be\n",
        "loaded as strings, not tensors:"
      ],
      "metadata": {
        "id": "lQundz74mqfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMDB"
      ],
      "metadata": {
        "id": "PZOaRVIenH9g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install beautifulsoup4\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "In [0]:\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "R6AvwGXhnI0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imdb_sentences = []\n",
        "train_data = tfds.as_numpy(tfds.load('imdb_reviews', split=\"train\"))\n",
        "for item in train_data:\n",
        "    imdb_sentences.append(str(item['text']))\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(imdb_sentences)\n",
        "sequences = tokenizer.texts_to_sequences(imdb_sentences)\n",
        "print(tokenizer.word_index)\n",
        "print(sequences[123])"
      ],
      "metadata": {
        "id": "wHp2qlSInTBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import string\n",
        "\n",
        "stopwords = [\"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\",\n",
        "             \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\",\n",
        "             \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\",\n",
        "             \"he\", \"hed\", \"hes\", \"her\", \"here\", \"heres\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\",\n",
        "             \"hows\", \"i\", \"id\", \"ill\", \"im\", \"ive\", \"if\", \"in\", \"into\", \"is\", \"it\", \"its\", \"itself\",\n",
        "             \"lets\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\",\n",
        "             \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"shed\", \"shell\", \"shes\", \"should\",\n",
        "             \"so\", \"some\", \"such\", \"than\", \"that\", \"thats\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\",\n",
        "             \"there\", \"theres\", \"these\", \"they\", \"theyd\", \"theyll\", \"theyre\", \"theyve\", \"this\", \"those\", \"through\",\n",
        "             \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"wed\", \"well\", \"were\", \"weve\", \"were\",\n",
        "             \"what\", \"whats\", \"when\", \"whens\", \"where\", \"wheres\", \"which\", \"while\", \"who\", \"whos\", \"whom\", \"why\",\n",
        "             \"whys\", \"with\", \"would\", \"you\", \"youd\", \"youll\", \"youre\", \"youve\", \"your\", \"yours\", \"yourself\",\n",
        "             \"yourselves\"]\n",
        "\n",
        "table = str.maketrans('', '', string.punctuation)\n",
        "imdb_sentences = []\n",
        "train_data = tfds.as_numpy(tfds.load('imdb_reviews', split=\"train\"))\n",
        "for item in train_data:\n",
        "    sentence = str(item['text'].decode('UTF-8').lower())\n",
        "    sentence = sentence.replace(\",\", \" , \")\n",
        "    sentence = sentence.replace(\".\", \" . \")\n",
        "    sentence = sentence.replace(\"-\", \" - \")\n",
        "    sentence = sentence.replace(\"/\", \" / \")\n",
        "    soup = BeautifulSoup(sentence)\n",
        "    sentence = soup.get_text()\n",
        "    words = sentence.split()\n",
        "    filtered_sentence = \"\"\n",
        "    for word in words:\n",
        "        word = word.translate(table)\n",
        "        if word not in stopwords:\n",
        "            filtered_sentence = filtered_sentence + word + \" \"\n",
        "    imdb_sentences.append(filtered_sentence)\n",
        "\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=25000)\n",
        "tokenizer.fit_on_texts(imdb_sentences)\n",
        "sequences = tokenizer.texts_to_sequences(imdb_sentences)\n",
        "print(tokenizer.word_index)"
      ],
      "metadata": {
        "id": "waGwO2VZnemj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\n",
        "    'Today is a sunny day',\n",
        "    'Today is a rainy day',\n",
        "    'Is it sunny today?'\n",
        "]\n",
        "In [0]:\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "print(sequences)\n",
        "In [0]:\n",
        "reverse_word_index = dict([(value, key) for (key, value) in tokenizer.word_index.items()])\n",
        "\n",
        "decoded_review = ' '.join([reverse_word_index.get(i, '?') for i in sequences[0]])\n",
        "\n",
        "print(decoded_review)"
      ],
      "metadata": {
        "id": "6xd3olvuoP08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Trying Subwords\n",
        "In [0]:\n",
        "(train_data, test_data), info = tfds.load(\n",
        "    # Use the version pre-encoded with an ~8k vocabulary.\n",
        "    'imdb_reviews/subwords8k', \n",
        "    # Return the train/test datasets as a tuple.\n",
        "    split = (tfds.Split.TRAIN, tfds.Split.TEST),\n",
        "    # Return (example, label) pairs from the dataset (instead of a dictionary).\n",
        "    as_supervised=True,\n",
        "    # Also return the `info` structure. \n",
        "    with_info=True)\n",
        "In [0]:\n",
        "encoder = info.features['text'].encoder\n",
        "print ('Vocabulary size: {}'.format(encoder.vocab_size))\n",
        "In [0]:\n",
        "print(encoder.decode([1, 2, 3]))\n",
        "In [0]:\n",
        "sample_string = 'Today is a sunny day'\n",
        "\n",
        "encoded_string = encoder.encode(sample_string)\n",
        "print ('Encoded string is {}'.format(encoded_string))\n",
        "\n",
        "original_string = encoder.decode(encoded_string)\n",
        "print ('The original string: \"{}\"'.format(original_string))\n",
        "\n",
        "assert original_string == sample_string\n",
        "In [0]:\n",
        "print(encoder.subwords[6426])"
      ],
      "metadata": {
        "id": "m2y5vST-ob-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Should you want to encode a string, you can do so with the encoder like this:\n",
        "sample_string = 'Today is a sunny day'\n",
        "encoded_string = encoder.encode(sample_string)\n",
        "print ('Encoded string is {}'.format(encoded_string))\n",
        "The output of this will be a list of tokens:\n",
        "Encoded string is [6427, 4869, 9, 4, 2365, 1361, 606]\n",
        "So, your five words are encoded into seven tokens. To see the tokens you can use the\n",
        "subwords property on the encoder, which returns an array. It’s zero-based, so whereas\n",
        "“Tod” in “Today” was encoded as 6427, it’s the 6,426th item in the array:\n",
        "print(encoder.subwords[6426])\n",
        "Tod\n",
        "If you want to decode, you can use the decode method of the encoder:\n",
        "encoded_string = encoder.encode(sample_string)\n",
        "original_string = encoder.decode(encoded_string)\n",
        "test_string = encoder.decode([6427, 4869, 9, 4, 2365, 1361, 606])\n",
        "The latter lines will have an identical result because encoded_string, despite its\n",
        "name, is a list of tokens just like the one that is hardcoded on the next line."
      ],
      "metadata": {
        "id": "HUa-pD8voywD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if you want to use\n",
        "28,000 sentences for training with the rest held back for testing, you can use code like\n",
        "this:\n",
        "training_size = 28000\n",
        "training_sentences = sentences[0:training_size]\n",
        "testing_sentences = sentences[training_size:]\n",
        "training_labels = labels[0:training_size]\n",
        "testing_labels = labels[training_size:]\n",
        "Now that you have a training set, you need to create the word index from it. Here is\n",
        "the code to use the tokenizer to create a vocabulary with up to 20,000 words. We’ll set\n",
        "the maximum length of a sentence at 10 words, truncate longer ones by cutting off\n",
        "the end, pad shorter ones at the end, and use “<OOV>”:\n",
        "vocab_size = 20000\n",
        "max_length = 10\n",
        "trunc_type='post'\n",
        "padding_type='post'\n",
        "oov_tok = \"<OOV>\"\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(training_sentences)\n",
        "word_index = tokenizer.word_index\n",
        "training_sequences = tokenizer.texts_to_sequences(training_sentences)\n",
        "training_padded = pad_sequences(training_sequences, maxlen=max_length,\n",
        "padding=padding_type,\n",
        "truncating=trunc_type)\n",
        "You can inspect the results by looking at training_sequences and training_padded.\n",
        "For example, here we print the first item in the training sequence, and you can see\n",
        "how it’s padded to a max length of 10:\n",
        "print(training_sequences[0])\n",
        "print(training_padded[0])\n",
        "[18, 3257, 47, 4770, 613, 508, 951, 423]\n",
        "[ 18 3257 47 4770 613 508 951 423 0 0]\n",
        "You can also inspect the word index by printing it:\n",
        "{'<OOV>': 1, 'just': 2, 'not': 3, 'now': 4, 'day': 5, 'get': 6, 'no': 7,\n",
        "'good': 8, 'like': 9, 'go': 10, 'dont': 11, ...}\n",
        "There are many words here you might want to consider getting rid of as stopwords,\n",
        "such as “like” and “dont.” It’s always useful to inspect the word index."
      ],
      "metadata": {
        "id": "MImq8thsoyy3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sarcasm Detection"
      ],
      "metadata": {
        "id": "kfC6-IAop8lV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install beautifulsoup4\n",
        "In [0]:\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "In [0]:\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "In [0]:\n",
        "from bs4 import BeautifulSoup\n",
        "import string\n",
        "\n",
        "stopwords = [\"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\",\n",
        "             \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\",\n",
        "             \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\",\n",
        "             \"he\", \"hed\", \"hes\", \"her\", \"here\", \"heres\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\",\n",
        "             \"hows\", \"i\", \"id\", \"ill\", \"im\", \"ive\", \"if\", \"in\", \"into\", \"is\", \"it\", \"its\", \"itself\",\n",
        "             \"lets\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\",\n",
        "             \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"shed\", \"shell\", \"shes\", \"should\",\n",
        "             \"so\", \"some\", \"such\", \"than\", \"that\", \"thats\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\",\n",
        "             \"there\", \"theres\", \"these\", \"they\", \"theyd\", \"theyll\", \"theyre\", \"theyve\", \"this\", \"those\", \"through\",\n",
        "             \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"wed\", \"well\", \"were\", \"weve\", \"were\",\n",
        "             \"what\", \"whats\", \"when\", \"whens\", \"where\", \"wheres\", \"which\", \"while\", \"who\", \"whos\", \"whom\", \"why\",\n",
        "             \"whys\", \"with\", \"would\", \"you\", \"youd\", \"youll\", \"youre\", \"youve\", \"your\", \"yours\", \"yourself\",\n",
        "             \"yourselves\"]\n",
        "\n",
        "table = str.maketrans('', '', string.punctuation)\n",
        "In [0]:\n",
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/sarcasm.json \\\n",
        "    -O /tmp/sarcasm.json\n",
        "  \n",
        "import json\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "with open(\"/tmp/sarcasm.json\", 'r') as f:\n",
        "    datastore = json.load(f)\n",
        "\n",
        "\n",
        "sentences = [] \n",
        "labels = []\n",
        "urls = []\n",
        "for item in datastore:\n",
        "    sentence = item['headline'].lower()\n",
        "    sentence = sentence.replace(\",\", \" , \")\n",
        "    sentence = sentence.replace(\".\", \" . \")\n",
        "    sentence = sentence.replace(\"-\", \" - \")\n",
        "    sentence = sentence.replace(\"/\", \" / \")\n",
        "    soup = BeautifulSoup(sentence)\n",
        "    sentence = soup.get_text()\n",
        "    words = sentence.split()\n",
        "    filtered_sentence = \"\"\n",
        "    for word in words:\n",
        "        word = word.translate(table)\n",
        "        if word not in stopwords:\n",
        "            filtered_sentence = filtered_sentence + word + \" \"\n",
        "    sentences.append(filtered_sentence)\n",
        "    labels.append(item['is_sarcastic'])\n",
        "    urls.append(item['article_link'])\n",
        "In [0]:\n",
        "print(len(sentences))\n",
        "In [0]:\n",
        "training_size = 23000\n",
        "\n",
        "training_sentences = sentences[0:training_size]\n",
        "testing_sentences = sentences[training_size:]\n",
        "training_labels = labels[0:training_size]\n",
        "testing_labels = labels[training_size:]\n",
        "In [0]:\n",
        "vocab_size = 20000\n",
        "max_length = 10\n",
        "trunc_type='post'\n",
        "padding_type='post'\n",
        "oov_tok = \"<OOV>\"\n",
        "\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(training_sentences)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "training_sequences = tokenizer.texts_to_sequences(training_sentences)\n",
        "padded = pad_sequences(training_sequences, padding='post')\n",
        "print(word_index)"
      ],
      "metadata": {
        "id": "x-uVzca4p_Ss"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}